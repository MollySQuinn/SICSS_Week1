# Exercises

## Day 2

1. Start a session with the tidyverse Wikipedia page. Adapt your user agent to some sort of different value. Proceed to Hadley Wickham's page. Go back. Go forth. Check the `session_history()` to see if it has worked.

```{r eval=FALSE}
library(rvest)

tidyverse_wiki <- "https://en.wikipedia.org/wiki/Tidyverse"
hadley_wiki <- "https://en.wikipedia.org/wiki/Hadley_Wickham"
user_agent <- httr::user_agent("Hi, I'm Felix and I try to steal your data.")

my_session <- session(tidyverse_wiki)
user_agent <- httr::user_agent("Mozilla/5.0 (Macintosh; Intel Mac OS X 12_0_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36")

library(magrittr)

my_session %<>% session_jump_to(hadley_wiki)
my_session

my_session %<>% 
  session_back()

my_session

my_session %<>% 
  session_forward()

my_session

my_session %>% session_history()

```

2. Start a session on "https://www.scrapethissite.com/pages/advanced/?gotcha=login", fill out, and submit the form. Any value for login and password will do. (Disclaimer: you have to add the URL as an "action" attribute after creating the form, see [this tutorial](https://github.com/tidyverse/rvest/issues/319). -- `login_form$action <- url`)

```{r eval=FALSE}
url <- "https://www.scrapethissite.com/pages/advanced/?gotcha=login"

#extract and set login form here
login_form <- read_html(url) %>% 
  html_form() %>% 
  pluck(1) %>% #another way to do [[1]]
  html_form_set(user = "mollz",
                pass = "myPass42")

login_form$action <- url # add url as action attribute

# resp <- html_form_submit(login_form)

# submit form
base_session <- session("https://www.scrapethissite.com/pages/advanced/?gotcha=login") %>% 
  session_submit(login_form, submit = 'Login →')

# resp <- html_form_submit(search)
# read_html(resp)
scrape_login_page <- base_session %>% 
  read_html()

scraped_html_text <- scrape_login_page %>% html_text()

grep("Successfully logged in! Nice job :)", scraped_html_text, value = TRUE)

```

3. Scrape 10 profile timelines from the following list. Check the [documentation](https://cran.r-project.org/web/packages/rtweet/vignettes/intro.html) for instructions.

```{r eval=FALSE}
library(rtweet)

uk_mps <- lists_members(
  list_id = "217199644"
) 
```

<!-- # Exercises -->

<!-- ## Day 2 -->

<!-- 1. Start a session with the tidyverse Wikipedia page. Adapt your user agent to some sort of different value. Proceed to Hadley Wickham's page. Go back. Go forth. Check the `session_history()` to see if it has worked. -->

<!-- ```{r eval=FALSE} -->
<!-- tidyverse_wiki <- "https://en.wikipedia.org/wiki/Tidyverse" -->
<!-- hadley_wiki <- "https://en.wikipedia.org/wiki/Hadley_Wickham" -->
<!-- user_agent <- httr::user_agent("Hi, I'm Felix and I try to steal your data.") -->
<!-- ``` -->

<!-- 2. Start a session on "https://www.scrapethissite.com/pages/advanced/?gotcha=login", fill out, and submit the form. Any value for login and password will do. (Disclaimer: you have to add the URL as an "action" attribute after creating the form, see [this tutorial](https://github.com/tidyverse/rvest/issues/319). -- `login_form$action <- url`) -->

<!-- ```{r eval=FALSE} -->
<!-- url <- "https://www.scrapethissite.com/pages/advanced/?gotcha=login" -->

<!-- #extract and set login form here -->

<!-- login_form$action <- url # add url as action attribute -->

<!-- # submit form -->
<!-- base_session <- session("https://www.scrapethissite.com/pages/advanced/?gotcha=login") %>%  -->
<!--   session_submit(login_form)  -->
<!-- ``` -->

<!-- 3. Scrape 10 profile timelines from the following list. Check the [documentation](https://cran.r-project.org/web/packages/rtweet/vignettes/intro.html) for instructions. -->

<!-- ```{r eval=FALSE} -->
<!-- library(rtweet) -->

<!-- uk_mps <- lists_members( -->
<!--   list_id = "217199644" -->
<!-- )  -->
<!-- ``` -->

## Day 3

1. Download all movies from the (IMDb Top250 best movies of all time list)[https://www.imdb.com/chart/top/?ref_=nv_mv_250]. Put them in a tibble with the columns `rank` -- in numeric format, `title`, `url` to IMDb entry, `rating` -- in numeric format.

```{r}
example_tbl <- tibble(
  a = 1:10,
  b = 11:20, # vectors must have the same length
  c = "c" # or length 1
)
example_tbl

# imdb <- read_html("https://www.imdb.com/chart/top/?ref_=nv_mv_250")
# 
# imbd_elements <- imdb %>% html_elements(xpath = "//td[@class='titleColumn']")
# 
# library(tidyverse)
# tibble(
#   tittle = imbd_elements %>% html_text2(),
#   link = imbd_elements %>% html_attr(name = "href")
# ) %>% 
#   glimpse()

imdb_top250 <- read_html("https://www.imdb.com/chart/top/?ref_=nv_mv_250")

tibble(
  rank = imdb_top250 %>%
    # html_elements(".titleColumn a") %>%
    html_elements(xpath = "//td[@class='titleColumn']") %>%
    html_text2() %>% 
    str_extract("^[0-9]+(?=\\.)") %>% 
    parse_integer(),
  title = imdb_top250 %>%
    # html_elements(".titleColumn a") %>%
    html_elements(xpath = "//td[@class='titleColumn']/a") %>%
    html_text2(),
  url = imdb_top250 %>% 
    html_elements(xpath = "//td[@class='titleColumn']/a") %>% 
    html_attr(name = "href"),
  rating = imdb_top250 %>%
    html_elements(xpath = "//td[@class='ratingColumn imdbRating']") %>%
    html_text2(),
    
)

```

2. Scrape the (British MPs Wikipedia page)[https://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_2019_United_Kingdom_general_election].
  a. Scrape the table containing data on all individual MPs. (Hint: you can select and remove single lines using `slice()`, e.g., `tbl %>% slice(1)` retains only the first line, `tbl %>% slice(-1)` removes only the first line.)
```{r}
tables <- read_html("https://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_2019_United_Kingdom_general_election") %>% 
  html_elements("#elected-mps") %>% 
  html_table() %>%
  pluck(1)

tables <- tables[1:650,]
```
  b. Extract the links linking to individual pages. Make them readable using `url_absolute()` – `url_absolute("/kdfnndfkok", base = "https://wikipedia.com")` --> `"https://wikipedia.com/kdfnndfkok"`. Add the links to the table from 1a.
```{r}
links <- read_html("https://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_2019_United_Kingdom_general_election") %>% 
  html_elements("#elected-mps a") %>%
  html_attr('href') %>%
  url_absolute("https://wikipedia.com/")

links
```
  c. Scrape the textual content of the **first** MP’s Wikipedia page.
```{r}

```
  d. (ADVANCED) Wrap 1c. in a function called `scrape_mp_wiki()`.
```{r}

```

3. Scrape the 10 first pages of [R-bloggers](https://www.r-bloggers.com) in an automated fashion. Make sure to take breaks between requests by including `Sys.sleep(2)`.
  a. Do so using the url vector we created.
  b. Do so by getting the link for the next page and a loop.
  c. Do so by using `html_session()` in a loop.
```{r}
for (i in 1:10){
  #write code here...
  Sys.sleep(2)
}
```


## Day 4

### Regexes

1.Write a regex for Swedish mobile number. Test it with `str_detect("+46 71-738 25 33", "[insert your regex here]")`. 
2. Find all Mercedes in the `mtcars` data set.
3. Take the IMDb file (`imdb <- read_csv("https://www.dropbox.com/s/81o3zzdkw737vt0/imdb2006-2016.csv?dl=1")`) and split the `Genre` column into different columns (hint: look at the `tidyr::separate()` function). How would you do it if `Genre` were a vector using `str_split_fixed()`?
4. Take this vector of heights: `heights <- c("1m30cm", "2m01cm", "3m10cm")`
    a. How can you extract the meters using the negative look behind?
    b. Bring it into numeric format (i.e., your_solution == c(1.3, 2.01, 3.1)) using regexes and `stringr` commands.

### Preprocessing and analyses

1. Download the Twitter timeline data (`timelines <- read_csv("https://www.dropbox.com/s/pat7muh816yhxlj/sample.csv?dl=1")`. Preprocess the Tweets. 
  a. Unnest the tokens.
  b. Remove stop words. 
  c. Perform stemming. 

2. Optional: Perform the same steps, but using `spacyr`. What works better, lemmatization or stemming?

3. Count the terms per party. 
  a. Do you see party-specific differences with regard to their ten most common terms (hint: `slice_max(tf_idf, n = 10, with_ties = FALSE)`)? Use the following code to plot them.
  b. Is there more words you should add to your stopwords list?
  c. Do the same thing but using the spacy output and filtering only `NOUN`s and `PROPN`ouns. 
  d. Again, is there stuff to be removed? Do so using a Regex.

4. Do the same thing as in 3. but use TF-IDF instead of raw counts. How does this alter your results?

5. What else could you have done in terms of preprocessing (think of the special characters and syntax Twitter uses here)?

## Day 5



## Day 6

No exercises.